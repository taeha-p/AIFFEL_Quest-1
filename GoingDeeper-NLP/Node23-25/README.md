Aiffelì—ì„œ ì§„í–‰í•œ GoingDeeper ë…¸ë“œê´€ë ¨ ì½”ë“œë“¤ì…ë‹ˆë‹¤!

----

STEP 1. NSMC ë°ì´í„° ë¶„ì„ ë° Huggingface dataset êµ¬ì„±   
- ë°ì´í„°ì…‹ì€ ê¹ƒí—ˆë¸Œì—ì„œ ë‹¤ìš´ë°›ê±°ë‚˜, Huggingface datasetsì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•ì—ì„œ ë°°ìš´ ë°©ë²•ë“¤ì„ í™œìš©í•´ë´…ì‹œë‹¤!    

STEP 2. klue/bert-base model ë° tokenizer ë¶ˆëŸ¬ì˜¤ê¸°    
STEP 3. ìœ„ì—ì„œ ë¶ˆëŸ¬ì˜¨ tokenizerìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ì „ì²˜ë¦¬í•˜ê³ , model í•™ìŠµ ì§„í–‰í•´ ë³´ê¸°   
STEP 4. Fine-tuningì„ í†µí•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥(accuarcy) í–¥ìƒì‹œí‚¤ê¸°   
ë°ì´í„° ì „ì²˜ë¦¬, TrainingArguments ë“±ì„ ì¡°ì •í•˜ì—¬ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ 90% ì´ìƒìœ¼ë¡œ ëŒì–´ì˜¬ë ¤ë´…ì‹œë‹¤.     

STEP 5. Bucketingì„ ì ìš©í•˜ì—¬ í•™ìŠµì‹œí‚¤ê³ , STEP 4ì˜ ê²°ê³¼ì™€ì˜ ë¹„êµ    
ì•„ë˜ ë§í¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ bucketingê³¼ dynamic paddingì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³´ê³ , ì´ë“¤ì„ ì ìš©í•˜ì—¬ modelì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.   

Data Collator   

Trainer.TrainingArguments ì˜ group_by_length   

STEP 4ì— í•™ìŠµí•œ ê²°ê³¼ì™€ bucketingì„ ì ìš©í•˜ì—¬ í•™ìŠµì‹œí‚¨ ê²°ê³¼ë¥¼ ë¹„êµí•´ë³´ê³ , ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒê³¼ í›ˆë ¨ ì‹œê°„ ë‘ ê°€ì§€ ì¸¡ë©´ì—ì„œ ê°ê° ì–´ë–¤ ì´ì ì´ ìˆëŠ”ì§€ ë¹„êµí•´ë´…ì‹œë‹¤.   

----


ì½”ë” : ê¹€ì§€ì›    

ë¦¬ë·°ì–´ : ì„œë¯¼ì„±

ğŸ”‘ **PRT(Peer Review Template)**

- [ ]  **1. ì£¼ì–´ì§„ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì™„ì„±ëœ ì½”ë“œê°€ ì œì¶œë˜ì—ˆë‚˜ìš”?**
    - ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ ì •ìƒì ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê³ , ì‘ë™í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤.(o)
        - klue/bert-baseë¥¼ NSMC ë°ì´í„°ì…‹ìœ¼ë¡œ fine-tuning í•˜ì—¬, ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤.
        - ë„¤ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.
    - Preprocessingì„ ê°œì„ í•˜ê³ , fine-tuningì„ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ ì‹œì¼°ë‹¤.(x)
        - Validation accuracyë¥¼ 90% ì´ìƒìœ¼ë¡œ ê°œì„ í•˜ì˜€ë‹¤.
        - fine-tuningì„ ì§„í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
    - ëª¨ë¸ í•™ìŠµì— Bucketingì„ ì„±ê³µì ìœ¼ë¡œ ì ìš©í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ ë¹„êµë¶„ì„í•˜ì˜€ë‹¤.(x)
        - Bucketing taskì„ ìˆ˜í–‰í•˜ì—¬ fine-tuning ì‹œ ì—°ì‚° ì†ë„ì™€ ëª¨ë¸ ì„±ëŠ¥ ê°„ì˜ trade-off ê´€ê³„ê°€ ë°œìƒí•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³ , ë¶„ì„í•œ ê²°ê³¼ë¥¼ ì œì‹œí•˜ì˜€ë‹¤.
        - í•´ë‹¹ taskì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

    
- [ ]  **2. ì „ì²´ ì½”ë“œì—ì„œ ê°€ì¥ í•µì‹¬ì ì´ê±°ë‚˜ ê°€ì¥ ë³µì¡í•˜ê³  ì´í•´í•˜ê¸° ì–´ë ¤ìš´ ë¶€ë¶„ì— ì‘ì„±ëœ 
ì£¼ì„ ë˜ëŠ” doc stringì„ ë³´ê³  í•´ë‹¹ ì½”ë“œê°€ ì˜ ì´í•´ë˜ì—ˆë‚˜ìš”?**
    - dataset êµ¬ì¶• Classì— ëŒ€í•œ ì„¤ëª…ì„ ì•ë‹¨ì— ì˜ ì ì–´ì£¼ì…¨ìŠµë‹ˆë‹¤.
    ```
    class DataSet():
    """
    Initialize: 
    - Initialize dataset object with pre-trained tokenizer.
    - padding set to maximum length
    - pre-trained tokenizer
    - incoming dataset should already been separated with
    three columns
    
    Transform fn:
    - Transform the data with auto tokenizer with self padding
    
    _set fn:
    - Train dataset will be further splited into 80-20.
    
    """
    def __init__(self, dataset_name, huggingface_tokenizer, padding='max_length'):
        super(DataSet, self).__init__()
        
        self.tokenizer = AutoTokenizer.from_pretrained(huggingface_tokenizer)
        
        self.padding = padding
        dataset = self._set(dataset_name)
        
        self.train = dataset['train']
        self.test = dataset['test']
        self.valid = dataset['valid']
                                        
            
    def transform(self, data):
        return self.tokenizer(
            data['document'],
            truncation=True,
            padding=self.padding,
            return_token_type_ids=False,
        )
       
        
    def _set(self, dataset_name):
        data = datasets.load_dataset(dataset_name) # dataset load
        train_valid = data['train'].train_test_split(test_size=0.2) # train_test_split
                
        return DatasetDict({
            'train': train_valid['train'],
            'valid': train_valid['test'],
            'test': data['test']
        }).map(self.transform, batched=True)
    ```
        
- [ ]  **3. ì—ëŸ¬ê°€ ë‚œ ë¶€ë¶„ì„ ë””ë²„ê¹…í•˜ì—¬ ë¬¸ì œë¥¼ â€œí•´ê²°í•œ ê¸°ë¡ì„ ë‚¨ê²¼ê±°ë‚˜â€ 
â€ìƒˆë¡œìš´ ì‹œë„ ë˜ëŠ” ì¶”ê°€ ì‹¤í—˜ì„ ìˆ˜í–‰â€í•´ë´¤ë‚˜ìš”?**
    - ëª¨ë¸ì— ëŒ€í•œ tokenizerì„ ì •ì— ëŒ€í•œ ê³ ë¯¼ì´ ëŠê»´ì§„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

        
- [ ]  **4. íšŒê³ ë¥¼ ì˜ ì‘ì„±í–ˆë‚˜ìš”?**
    - íšŒê³ ëŠ” ì‘ì„±ë˜ì–´ ìˆì§€ëŠ” ì•Šì•˜ìŠµë‹ˆë‹¤.


- [ ]  **5. ì½”ë“œê°€ ê°„ê²°í•˜ê³  íš¨ìœ¨ì ì¸ê°€ìš”?**
    - ëª¨ë“ˆí™”ë¥¼ ì§„í–‰í•˜ì—¬ ê°„ê²°í•˜ê²Œ ì½”ë“œë¥¼ ì‘ì„±í•˜ë ¤í–ˆìŠµë‹ˆë‹¤.
      ```
      class DataSet():
    """
    Initialize: 
    - Initialize dataset object with pre-trained tokenizer.
    - padding set to maximum length
    - pre-trained tokenizer
    - incoming dataset should already been separated with
    three columns
    
    Transform fn:
    - Transform the data with auto tokenizer with self padding
    
    _set fn:
    - Train dataset will be further splited into 80-20.
    
    """
    def __init__(self, dataset_name, huggingface_tokenizer, padding='max_length'):
        super(DataSet, self).__init__()
        
        self.tokenizer = AutoTokenizer.from_pretrained(huggingface_tokenizer)
        
        self.padding = padding
        dataset = self._set(dataset_name)
        
        self.train = dataset['train']
        self.test = dataset['test']
        self.valid = dataset['valid']
                                        
            
    def transform(self, data):
        return self.tokenizer(
            data['document'],
            truncation=True,
            padding=self.padding,
            return_token_type_ids=False,
        )
       
        
    def _set(self, dataset_name):
        data = datasets.load_dataset(dataset_name) # dataset load
        train_valid = data['train'].train_test_split(test_size=0.2) # train_test_split
                
        return DatasetDict({
            'train': train_valid['train'],
            'valid': train_valid['test'],
            'test': data['test']
        }).map(self.transform, batched=True)
       ```


Reference:

